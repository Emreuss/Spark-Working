{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycef\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/emre/spark-3.0.1-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_sql_kafka = \"/home/emre/spark-3.0.1-bin-hadoop2.7/jars/spark-sql-kafka-0-10_2.12-3.0.1.jar\"\n",
    "kafka_clients = \"/home/emre/spark-3.0.1-bin-hadoop2.7/jars/kafka-clients-2.6.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic_name = \"cef-topic-kafka\"\n",
    "kafka_bootstrap_servers = 'localhost:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"spark-kafka2\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.jars\", spark_sql_kafka) \\\n",
    "        .config(\"spark.jars\", kafka_clients) \\\n",
    "        .config(\"spark.driver.extraClassPath\",\"/home/emre/spark-3.0.1-bin-hadoop2.7/jars/*.jar\") \\\n",
    "        .config(\"spark.executor.extraClassPath\",\"/home/emre/spark-3.0.1-bin-hadoop2.7/jars/*.jar\") \\\n",
    "        .config(\"spark.jars\", \"/home/emre/spark-3.0.1-bin-hadoop2.7/jars/postgresql-42.3.1.jar\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "        .option(\"subscribe\", kafka_topic_name) \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f02d3ef2c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.selectExpr(\"CAST(key as STRING)\",\"CAST(value as STRING)\") \\\n",
    ".writeStream \\\n",
    ".format(\"memory\") \\\n",
    ".queryName(\"ceftopic\") \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql(\"SELECT value FROM ceftopic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.select([col(value).cast(\"string\") for value in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "|CEF:0|Citrix|NetS...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " for f in df2.collect(): \n",
    "        #print (pycef.parse(f.value))\n",
    "        b.append(pycef.parse(f.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(Row(**x) for x in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DeviceVendor: string, DeviceProduct: string, DeviceVersion: string, DeviceEventClassID: string, Name: string, DeviceName: string, Severity: string, DeviceSeverity: string, CEFVersion: string, src: string, spt: string, method: string, request: string, msg: string, cn1: string, cn2: string, cs1: string, cs2: string, cs3: string, cs4: string, cs5: string, act: string]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF_LST=sparkDF \\\n",
    ".withColumn(\"Severity\",sparkDF[\"Severity\"].cast(\"integer\")) \\\n",
    ".withColumn(\"DeviceSeverity\",sparkDF[\"DeviceSeverity\"].cast(\"integer\")) \\\n",
    ".withColumn(\"CEFVersion\",sparkDF[\"CEFVersion\"].cast(\"integer\")) \\\n",
    ".withColumn(\"spt\",sparkDF[\"spt\"].cast(\"integer\")) \\\n",
    ".withColumn(\"cn1\",sparkDF[\"cn1\"].cast(\"integer\")) \\\n",
    ".withColumn(\"cn2\",sparkDF[\"cn2\"].cast(\"integer\")) \\\n",
    ".withColumn(\"cs5\",sparkDF[\"cs5\"].cast(\"integer\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DeviceVendor: string, DeviceProduct: string, DeviceVersion: string, DeviceEventClassID: string, Name: string, DeviceName: string, Severity: int, DeviceSeverity: int, CEFVersion: int, src: string, spt: int, method: string, request: string, msg: string, cn1: int, cn2: int, cs1: string, cs2: string, cs3: string, cs4: string, cs5: int, act: string]>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF_LST.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+------------------+--------------+--------------+--------+--------------+----------+-------------+-----+------+--------------------+--------------------+---+---+--------+----+--------------------+-----+----+-------+\n",
      "|DeviceVendor|DeviceProduct|DeviceVersion|DeviceEventClassID|          Name|    DeviceName|Severity|DeviceSeverity|CEFVersion|          src|  spt|method|             request|                 msg|cn1|cn2|     cs1| cs2|                 cs3|  cs4| cs5|    act|\n",
      "+------------+-------------+-------------+------------------+--------------+--------------+--------+--------------+----------+-------------+-----+------+--------------------+--------------------+---+---+--------+----+--------------------+-----+----+-------+\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "|      Citrix|    NetScaler|       NS10.0|             APPFW|APPFW_STARTURL|APPFW_STARTURL|       6|             6|         0|10.217.253.78|53743|   GET|http://vpx247.exa...|Disallow Illegal ...|233|205|profile1|PPE0|AjSZM26h2M+xL809p...|ALERT|2012|blocked|\n",
      "+------------+-------------+-------------+------------------+--------------+--------------+--------+--------------+----------+-------------+-----+------+--------------------+--------------------+---+---+--------+----+--------------------+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF_LST.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://localhost:5432/ceftopic\"\n",
    "properties = {\"user\": \"postgres\",\"password\": \"*****\",\"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF_LST \\\n",
    ".write.\\\n",
    "jdbc(url=url, table=\"log_\", mode=mode, properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
